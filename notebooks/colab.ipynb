{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency Models, but with JAX ðŸŽ†\n",
    "\n",
    "Improved Consistency Training ([arxiv.org/abs/2310.14189](https://arxiv.org/abs/2310.14189)) with JAX & Flax, partially based on [smsharma/consistency-models](https://github.com/smsharma/consistency-models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install jax flax torch torchvision ipykernel\n",
    "%pip install ml_collections matplotlib einops wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import einops\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax.training.train_state import TrainState\n",
    "import flax.linen as nn\n",
    "\n",
    "import optax\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from jax.tree_util import tree_map\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional, Sequence, Callable, Any\n",
    "from dataclasses import dataclass\n",
    "from abc import abstractmethod\n",
    "\n",
    "Shape = int | Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.Dense(self.mlp_dim)(x)\n",
    "        y = nn.gelu(y)\n",
    "        return nn.Dense(x.shape[-1])(y)\n",
    "\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        y = MLPBlock(self.tokens_mlp_dim)(y)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        x = x + y\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = MLPBlock(self.channels_mlp_dim)(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    \"\"\"MLP-Mixer architecture from https://arxiv.org/abs/2105.01601.\"\"\"\n",
    "\n",
    "    patch_size: int\n",
    "    num_blocks: int\n",
    "    hidden_dim: int\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "    num_classes: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t, context):\n",
    "        b, h, w, c = x.shape\n",
    "\n",
    "        d_t_emb = t.shape[-1]\n",
    "\n",
    "        context = nn.Embed(self.num_classes, t.shape[-1])(context)\n",
    "        context = einops.repeat(context, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "\n",
    "        t = einops.repeat(t, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "        context = jnp.concatenate([context, t], axis=-1)\n",
    "\n",
    "        context = nn.gelu(nn.Dense(self.tokens_mlp_dim)(context))\n",
    "        context = nn.Dense(d_t_emb)(context)\n",
    "\n",
    "        x = jnp.concatenate([x, context], axis=-1)\n",
    "\n",
    "        x = nn.Conv(self.hidden_dim, [self.patch_size, self.patch_size], strides=[self.patch_size, self.patch_size])(x)\n",
    "        x = einops.rearrange(x, \"n h w c -> n (h w) c\")\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = MixerBlock(self.tokens_mlp_dim, self.channels_mlp_dim)(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = nn.Dense(self.patch_size * self.patch_size * c)(x)\n",
    "        x = einops.rearrange(x, \"b (hp wp) (ph pw c) -> b (hp ph) (wp pw) c\", hp=h // self.patch_size, wp=w // self.patch_size, ph=self.patch_size, pw=self.patch_size, c=c)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size: int\n",
    "    input_size: int\n",
    "    channel_size: int\n",
    "    time_embed_size: int\n",
    "    dtype: Any = jnp.float32\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_backbone(self) -> nn.Module:\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class MLPMixerConfig(ModelConfig):\n",
    "    patch_size: int = 4\n",
    "    num_blocks: int = 4\n",
    "    hidden_dim: int = 256\n",
    "    tokens_mlp_dim: int = 256\n",
    "    channels_mlp_dim: int = 256\n",
    "    num_classes: int = 10\n",
    "    \n",
    "    def _create_backbone(self):\n",
    "        return MLPMixer(patch_size=self.patch_size,\n",
    "                        num_blocks=self.num_blocks,\n",
    "                        hidden_dim=self.hidden_dim,\n",
    "                        tokens_mlp_dim=self.tokens_mlp_dim,\n",
    "                        channels_mlp_dim=self.channels_mlp_dim,\n",
    "                        num_classes=self.num_classes)\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    lr: float = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyModel:\n",
    "    def __init__(self, config: ModelConfig, key: jax.dtypes.prng_key):\n",
    "        self.config = config\n",
    "        self.random_key = key\n",
    "        self.backbone = config._create_backbone()\n",
    "\n",
    "    def create_state(self, key: jax.dtypes.prng_key, optimizer: Optional[Any] = None, keep_state: bool = True):\n",
    "        noise_key, init_key = random.split(key)\n",
    "        init_shape = (self.config.batch_size,\n",
    "                      self.config.input_size,\n",
    "                      self.config.input_size,\n",
    "                      self.config.channel_size)\n",
    "        x = jax.random.normal(noise_key, init_shape)\n",
    "        t = jnp.ones((self.config.batch_size, self.config.time_embed_size))\n",
    "        \n",
    "        context = jnp.ones((self.config.batch_size,)).astype(jnp.int32)\n",
    "        params = self.backbone.init(init_key, x, t, context)\n",
    "        train_state = TrainState.create(apply_fn=self.backbone.apply, params=params, tx=optimizer)\n",
    "\n",
    "        if keep_state:\n",
    "            self.state = train_state\n",
    "    \n",
    "    def sample(self, timesteps: Sequence):\n",
    "        initial_variance = self.config.max_time ** 2\n",
    "\n",
    "        self.random_key, normal_key = random.split(self.random_key)\n",
    "        xt = random.normal(normal_key, self.config.dim) * initial_variance\n",
    "\n",
    "        x = self.consistency_fn(xt, self.config.max_time, self.backbone.apply_fn, self.backbone.params, self.config.sigma_data, self.config.eps)\n",
    "\n",
    "        for t in timesteps:\n",
    "            normal_key, _ = random.split(normal_key)\n",
    "            z = random.normal(normal_key, self.config.dim)\n",
    "            xt = x + jnp.sqrt(t**2 - self.config.eps**2) * z\n",
    "            x = self.consistency_fn(xt, self.config.max_time, self.model.apply_fn, self.model.params, self.config.sigma_data, self.config.eps)\n",
    "\n",
    "    def param_count(self):\n",
    "        if self.train_state is None:\n",
    "            raise \"Model not initialized. Call `create_state` first to initialize.\"\n",
    "        \n",
    "        return sum(x.size for x in jax.tree_leaves(self.state.params))\n",
    "    \n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(1, 4, 5))\n",
    "    def consistency_fn(apply_params: Any,\n",
    "                       apply_fn: Callable,\n",
    "                       x: jax.Array,\n",
    "                       timestep_emb: jax.Array,\n",
    "                       sigma_data: float,\n",
    "                       eps: float):\n",
    "        \n",
    "        cskip = lambda t: sigma_data ** 2 / ((t-eps)**2 + sigma_data ** 2)[:, :, None, None]\n",
    "        cout = lambda t: sigma_data * (t-eps) / jnp.sqrt(t**2 + sigma_data ** 2)[:, :, None, None]\n",
    "\n",
    "        x = apply_fn(apply_params, x, timestep_emb)\n",
    "\n",
    "        return x * cskip(timestep_emb) + x * cout(timestep_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConsistencyTrainer:\n",
    "    model: ConsistencyModel\n",
    "    dataloader: DataLoader\n",
    "    config: TrainerConfig\n",
    "    random_key: jax.dtypes.prng_key\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        if isinstance(self.dataloader, DataLoader):\n",
    "            self.dataloader = iter(self.dataloader)\n",
    "\n",
    "        self.random_key, init_key = jax.random.split(self.random_key)\n",
    "        tx = optax.radam(learning_rate=self.config.lr)\n",
    "        self.train_state = self.model.create_state(optimizer=tx, key=init_key)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: loss_fn and train_step are full of shit and aren't iCT.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(5,))\n",
    "    def loss_fn(params, consistency_fn, x, t1, t2, score, key, y):\n",
    "        z = jax.random.normal(key, shape=x.shape)\n",
    "\n",
    "        x2 = x + z * t2[:, :, None, None]\n",
    "        x2 = consistency_fn(params, score, x2, t2, y)\n",
    "\n",
    "        x1 = x + z * t1[:, :, None, None]\n",
    "        x1 = consistency_fn(score, x1, t1, y)\n",
    "\n",
    "        return np.mean((x1 - x2) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(5, 6, 7, 8, 9, 10))\n",
    "    def train_step(train_state: TrainState,\n",
    "                   batch: jax.Array,\n",
    "                   t1: jax.Array,\n",
    "                   t2: jax.Array,\n",
    "                   key: jax.dtypes.prng_key,\n",
    "                   model: ConsistencyModel,\n",
    "                   loss_fn: Callable,\n",
    "                   sigma_data: float,\n",
    "                   eps: float,\n",
    "                   time_embed_size: float):\n",
    "                   \n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "        \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(train_state.params, x_batch, t1, t2, model, key, y_batch, sigma_data, eps, time_embed_size)\n",
    "        grads = jax.lax.pmean(grads, \"batch\")\n",
    "        loss = jax.lax.pmean(loss, \"batch\")\n",
    "\n",
    "        train_state = train_state.apply_gradients(grads=grads)\n",
    "\n",
    "        metrics = {\"loss\": loss}\n",
    "\n",
    "        return train_state, metrics\n",
    "\n",
    "    def train(self, timesteps: int):\n",
    "        max_steps = self.model.config.max_steps\n",
    "        state = self.train_state\n",
    "        \n",
    "        with trange(timesteps) as steps:\n",
    "            for step in steps:\n",
    "                N = jnp.ceil(jnp.sqrt((step * ((s1 + 1) ** 2 - s0 ** 2) / max_steps) + s0 ** 2) - 1) + 1\n",
    "                mu = jnp.exp(s0 * jnp.log(mu0) / N)\n",
    "                boundaries = timestep_discretization(sigma, eps, N, T)\n",
    "\n",
    "                x_batch, y_batch = next(self.dataloader)\n",
    "                x_batch, y_batch = x_batch._numpy(), y_batch._numpy()\n",
    "                \n",
    "                key, _ = jax.random.split(key)\n",
    "                n_batch = jax.random.randint(key, minval=0, maxval=N - 1, shape=(x_batch.shape[0], 1))\n",
    "                \n",
    "                key, _ = jax.random.split(key)\n",
    "                state, loss = self.train_step(state,\n",
    "                                              (x_batch, y_batch),\n",
    "                                              boundaries[n_batch],\n",
    "                                              boundaries[n_batch + 1],\n",
    "                                              key,\n",
    "                                              score,\n",
    "                                              self.loss_fn)\n",
    "\n",
    "                steps.set_postfix(val=loss)\n",
    "                \n",
    "        self.train_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pmapped function has static_broadcasted_argnums=(5, 6, 7, 8, 9, 10) but was called with only 3 positional arguments. All static broadcasted arguments must be passed positionally.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 24\u001b[0m\n\u001b[1;32m     14\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mmnist_dataset,\n\u001b[1;32m     15\u001b[0m                          batch_size\u001b[38;5;241m=\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     16\u001b[0m                          shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m                          collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ConsistencyTrainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m                              dataloader\u001b[38;5;241m=\u001b[39mdataloader,\n\u001b[1;32m     21\u001b[0m                              config\u001b[38;5;241m=\u001b[39mTrainerConfig(),\n\u001b[1;32m     22\u001b[0m                              random_key\u001b[38;5;241m=\u001b[39mtrainer_key)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[105], line 38\u001b[0m, in \u001b[0;36mConsistencyTrainer.train\u001b[0;34m(self, timesteps)\u001b[0m\n\u001b[1;32m     35\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader)\n\u001b[1;32m     36\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state \u001b[38;5;241m=\u001b[39m next_state\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/projects/mincy/.venv/lib/python3.10/site-packages/jax/_src/api.py:1641\u001b[0m, in \u001b[0;36m_prepare_pmap\u001b[0;34m(fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, in_devices, backend_name, axis_size, args, kwargs)\u001b[0m\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_broadcasted_tuple:\n\u001b[1;32m   1640\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(static_broadcasted_tuple) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args):\n\u001b[0;32m-> 1641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpmapped function has static_broadcasted_argnums=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatic_broadcasted_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1643\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but was called with only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1644\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m>\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1645\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll static broadcasted arguments must be passed positionally.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1646\u001b[0m   dyn_argnums \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))\n\u001b[1;32m   1647\u001b[0m                  \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m static_broadcasted_tuple]\n\u001b[1;32m   1648\u001b[0m   f, dyn_args \u001b[38;5;241m=\u001b[39m argnums_partial(f, dyn_argnums, args)\n",
      "\u001b[0;31mValueError\u001b[0m: pmapped function has static_broadcasted_argnums=(5, 6, 7, 8, 9, 10) but was called with only 3 positional arguments. All static broadcasted arguments must be passed positionally."
     ]
    }
   ],
   "source": [
    "random_key = random.key(0)\n",
    "random_key, model_key, trainer_key = random.split(random_key, 3)\n",
    "\n",
    "model_config = MLPMixerConfig(batch_size=4,\n",
    "                        input_size=28, \n",
    "                        channel_size=1,\n",
    "                        time_embed_size=16)\n",
    "\n",
    "model = ConsistencyModel(model_config, model_key)\n",
    "\n",
    "transform = Compose([ToTensor()])\n",
    "collate_fn = lambda x: tree_map(np.asarray, default_collate(x))\n",
    "mnist_dataset = MNIST('/tmp/mnist', download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset=mnist_dataset,\n",
    "                         batch_size=model_config.batch_size,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=collate_fn)\n",
    "\n",
    "trainer = ConsistencyTrainer(model=model,\n",
    "                             dataloader=dataloader,\n",
    "                             config=TrainerConfig(),\n",
    "                             random_key=trainer_key)\n",
    "\n",
    "trainer.train(timesteps=int(1e4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
