{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Techniques for Training Consistency Models\n",
    "\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2310.14189-b31b1b.svg)](https://arxiv.org/abs/2310.14189)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/leakedweights/mincy/blob/main/notebooks/colab.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "JAX & Flax implementation of [Improved Consistency Training](https://arxiv.org/abs/2310.14189).\n",
    "This code is partially based on [smsharma/consistency-models](https://github.com/smsharma/consistency-models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade jax flax torch torchvision ipykernel einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚚 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import einops\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from jax.scipy.special import erf\n",
    "from jax.scipy.stats import multinomial\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "\n",
    "import optax\n",
    "import numpy as np\n",
    "from optax.losses import l2_loss\n",
    "from jax.tree_util import tree_map\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from abc import abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎲 Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.Dense(self.mlp_dim)(x)\n",
    "        y = nn.gelu(y)\n",
    "        return nn.Dense(x.shape[-1])(y)\n",
    "\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        y = MLPBlock(self.tokens_mlp_dim)(y)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        x = x + y\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = MLPBlock(self.channels_mlp_dim)(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    patch_size: int\n",
    "    num_blocks: int\n",
    "    hidden_dim: int\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "    num_classes: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t, context):\n",
    "        b, h, w, c = x.shape\n",
    "        d_t_emb = t.shape[-1]\n",
    "\n",
    "        context = nn.Embed(self.num_classes, t.shape[-1])(context)\n",
    "        context = einops.repeat(context, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "\n",
    "        t = einops.repeat(t, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "        context = jnp.concatenate([context, t], axis=-1)\n",
    "\n",
    "        context = nn.gelu(nn.Dense(self.tokens_mlp_dim)(context))\n",
    "        context = nn.Dense(d_t_emb)(context)\n",
    "\n",
    "        x = jnp.concatenate([x, context], axis=-1)\n",
    "\n",
    "        x = nn.Conv(self.hidden_dim, [self.patch_size, self.patch_size], strides=[self.patch_size, self.patch_size])(x)\n",
    "        x = einops.rearrange(x, \"n h w c -> n (h w) c\")\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = MixerBlock(self.tokens_mlp_dim, self.channels_mlp_dim)(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = nn.Dense(self.patch_size * self.patch_size * c)(x)\n",
    "        x = einops.rearrange(x, \"b (hp wp) (ph pw c) -> b (hp ph) (wp pw) c\", hp=h // self.patch_size, wp=w // self.patch_size, ph=self.patch_size, pw=self.patch_size, c=c)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import multiprocessing\n",
    "\n",
    "# hardware = \"tpu\"\n",
    "# hardware = \"gpu\"\n",
    "hardware = \"cpu\"\n",
    "\n",
    "if hardware == \"tpu\":\n",
    "    pass # setup tpu\n",
    "\n",
    "elif hardware == \"gpu\":\n",
    "    pass # setup gpu\n",
    "\n",
    "else:\n",
    "    os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(\n",
    "        multiprocessing.cpu_count()\n",
    "    )\n",
    "\n",
    "    jax.config.update('jax_platform_name', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size: int\n",
    "    input_size: int\n",
    "    channel_size: int\n",
    "    time_embed_size: int\n",
    "    max_time: float = 80.0\n",
    "    s0: int = 10\n",
    "    s1: int = 1280\n",
    "    c=5.4e-4\n",
    "    eps: float = 2e-3\n",
    "    sigma: float = 0.5\n",
    "    rho: float = 7.0\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80\n",
    "    p_mean: float = -1.1\n",
    "    p_std: float = 2.0\n",
    "    dtype: Any = jnp.float32\n",
    "\n",
    "    def __post_init__(self):\n",
    "        data_dim = self.channel_size * self.input_size ** 2\n",
    "        self.c_data = float(self.c * jnp.sqrt(data_dim))\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_backbone(self) -> nn.Module:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MLPMixerConfig(ModelConfig):\n",
    "    patch_size: int = 4\n",
    "    num_blocks: int = 4\n",
    "    hidden_dim: int = 256\n",
    "    tokens_mlp_dim: int = 256\n",
    "    channels_mlp_dim: int = 256\n",
    "    num_classes: int = 10\n",
    "    \n",
    "    def _create_backbone(self):\n",
    "        return MLPMixer(patch_size=self.patch_size,\n",
    "                        num_blocks=self.num_blocks,\n",
    "                        hidden_dim=self.hidden_dim,\n",
    "                        tokens_mlp_dim=self.tokens_mlp_dim,\n",
    "                        channels_mlp_dim=self.channels_mlp_dim,\n",
    "                        num_classes=self.num_classes)\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    lr: float = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✨ Consistency Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_huber_loss(x: jax.Array, y: jax.Array, c_data: float):\n",
    "    loss = l2_loss(x, y)\n",
    "    loss = jnp.sqrt(loss + c_data**2) - c_data\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(time: jax.Array, embedding_size: int):\n",
    "    time = time[..., 0]\n",
    "    time = time * 1e3\n",
    "    half_dim = embedding_size // 2\n",
    "    emb_scale = jnp.log(1e4) / (half_dim - 1)\n",
    "    \n",
    "    emb = jnp.arange(half_dim) * -emb_scale\n",
    "    emb = jnp.exp(emb)\n",
    "    emb = emb[None, :] * time[:, None]\n",
    "\n",
    "    sin_emb = jnp.sin(emb)\n",
    "    cos_emb = jnp.cos(emb)\n",
    "    embedding = jnp.concatenate([sin_emb, cos_emb], axis=-1)\n",
    "\n",
    "    if embedding_size % 2 == 1:\n",
    "        padding = ((0, 0), (0, 0), (0, 1))\n",
    "        embedding = jnp.pad(embedding, padding, mode='constant')\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyModel:\n",
    "    def __init__(self, config: ModelConfig, key: jax.dtypes.prng_key):\n",
    "        self.config = config\n",
    "        self.random_key = key\n",
    "        self.backbone = config._create_backbone()\n",
    "    \n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(3, 4))\n",
    "    def consistency_fn(x: jax.Array,\n",
    "                       h: jax.Array,\n",
    "                       t: float,\n",
    "                       sigma: float,\n",
    "                       eps: float):\n",
    "        \n",
    "        cskip = lambda t: (sigma ** 2 / ((t-eps)**2 + sigma ** 2))[:, :, None, None]\n",
    "        cout = lambda t: (sigma * (t-eps) / jnp.sqrt(t**2 + sigma ** 2))[:, :, None, None]\n",
    "        return x * cskip(t) + h * cout(t)\n",
    "    \n",
    "    def get_param_count(self):\n",
    "        if self.train_state is None:\n",
    "            raise \"Model not initialized. Call `create_state` first to initialize.\"\n",
    "        \n",
    "        return sum(x.size for x in jax.tree_leaves(self.state.params))\n",
    "    \n",
    "    def ict_discretize(self, step: int):\n",
    "        u = jnp.log(jnp.floor(self.config.s1 / self.config.s0)) + 1\n",
    "        k_prime = jnp.floor(self.config.max_time / u)\n",
    "\n",
    "        N = self.config.s0 * jnp.pow(2, jnp.floor(step / k_prime))\n",
    "        N = min(self.config.s1, N)\n",
    "\n",
    "        return int(N)\n",
    "\n",
    "    def karras_levels(self, N):\n",
    "            rho = self.config.rho\n",
    "            sigma_min = self.config.sigma_min\n",
    "            sigma_max = self.config.sigma_max\n",
    "            idx = jnp.arange(0, N-1)\n",
    "\n",
    "            sigma_i = jnp.pow(sigma_min, 1.0 / rho) \n",
    "            sigma_i = sigma_i + (idx-1) / (N-1) * (jnp.pow(sigma_max, 1.0 / rho) - jnp.pow(sigma_min, 1.0 / rho) )\n",
    "            sigma_i = jnp.pow(sigma_i, rho)\n",
    "            return sigma_i\n",
    "\n",
    "    def sample_timesteps(self, key, noise_levels, shape):\n",
    "            sigma_erf = lambda sigma: (jnp.log(sigma[1:]) - self.config.p_mean) / (jnp.sqrt(2) * self.config.p_std)\n",
    "\n",
    "            index_erfs = sigma_erf(noise_levels[1:]) - sigma_erf(noise_levels[:-1])\n",
    "            probs = index_erfs / jnp.sum(index_erfs)\n",
    "            timesteps = random.choice(key, len(probs), p=probs, replace=True, shape=shape)\n",
    "            \n",
    "            t1 = noise_levels[timesteps]\n",
    "            t2 = noise_levels[timesteps + 1]\n",
    "            return t1, t2\n",
    "    \n",
    "    def create_state(self, key: jax.dtypes.prng_key, optimizer: Optional[Any] = None, keep_state: bool = True):\n",
    "        noise_key, init_key = random.split(key)\n",
    "        device_batch_size = self.config.batch_size// jax.device_count()\n",
    "        init_shape = (device_batch_size,\n",
    "                      self.config.input_size,\n",
    "                      self.config.input_size,\n",
    "                      self.config.channel_size)\n",
    "        x = jax.random.normal(noise_key, init_shape)\n",
    "        t = jnp.ones((device_batch_size, self.config.time_embed_size))\n",
    "        \n",
    "        y = jnp.ones((device_batch_size,)).astype(jnp.int32)\n",
    "        params = self.backbone.init(init_key, x, t, y)\n",
    "        train_state = TrainState.create(apply_fn=self.backbone.apply, params=params, tx=optimizer)\n",
    "\n",
    "        if keep_state:\n",
    "            self.state = train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConsistencyTrainer:\n",
    "    model: ConsistencyModel\n",
    "    dataloader: DataLoader\n",
    "    config: TrainerConfig\n",
    "    random_key: jax.dtypes.prng_key\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.random_key, init_key = jax.random.split(self.random_key)\n",
    "        tx = optax.radam(learning_rate=self.config.lr)\n",
    "        self.model.create_state(optimizer=tx, key=init_key)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(1, 2, 6, 9, 10, 11))\n",
    "    def loss_fn(params, apply_fn, consistency_fn, x, t1, t2, time_embed_size, y, key, sigma, eps, c_data):\n",
    "        z = jax.random.normal(key, shape=x.shape)\n",
    "\n",
    "        x1 = x + z * t1[:, :, None, None]\n",
    "        t1_emb = sinusoidal_embedding(t1, time_embed_size)\n",
    "\n",
    "        h1 = apply_fn(params, x, t1_emb, y)\n",
    "        x1 = consistency_fn(x1, h1, t1, sigma, eps)\n",
    "\n",
    "        x2 = x + z * t2[:, :, None, None]\n",
    "        t2_emb = sinusoidal_embedding(t2, time_embed_size)\n",
    "\n",
    "        h2 = apply_fn(params, x, t2_emb, y)\n",
    "        x2 = consistency_fn(x2, h2, t2, sigma, eps)\n",
    "\n",
    "        loss = pseudo_huber_loss(x1, x2, c_data)\n",
    "        weight = (1 / (t2 - t1))[:, :, None, None]\n",
    "\n",
    "        return jnp.mean(weight * loss)\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(5, 6, 7, 8, 9, 10))\n",
    "    def train_step(state, batch, t1, t2, key, consistency_fn, loss_fn, time_embed_size, sigma, eps, c_data):\n",
    "        x, y = batch\n",
    "        params = state.params\n",
    "        apply_fn = state.apply_fn\n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, apply_fn, consistency_fn, x, t1, t2, time_embed_size, y, key, sigma, eps, c_data)\n",
    "\n",
    "        grads = jax.lax.pmean(grads, \"batch\")\n",
    "        loss = jax.lax.pmean(loss, \"batch\")\n",
    "\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss\n",
    "\n",
    "    def train(self, timesteps: int):\n",
    "        state = replicate(self.model.state)\n",
    "        batch_size = self.model.config.batch_size\n",
    "        num_devices = jax.device_count()\n",
    "        device_batch_size = batch_size // num_devices\n",
    "\n",
    "        assert batch_size % num_devices == 0, \\\n",
    "            f\"Batch size must be divisible by the number of devices, but got {batch_size} and {num_devices}.\"\n",
    "        \n",
    "        with trange(timesteps) as steps:\n",
    "            for step in steps:\n",
    "                try:\n",
    "                    batch = next(self.dataloader.__iter__())\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "                \n",
    "                x_batch, y_batch = batch\n",
    "\n",
    "                b, h, w, c = x_batch.shape\n",
    "                x_batch = x_batch.reshape(num_devices, device_batch_size, h, w, c)\n",
    "                y_batch = y_batch.reshape(num_devices, device_batch_size)\n",
    "\n",
    "                sigma = self.model.config.sigma\n",
    "                eps = self.model.config.eps\n",
    "                c_data = self.model.config.c_data\n",
    "                self.random_key, time_key = random.split(self.random_key)\n",
    "                self.random_key, *train_keys = random.split(self.random_key, min(x_batch.shape[0], jax.local_device_count()) + 1)\n",
    "\n",
    "                N = self.model.ict_discretize(step)\n",
    "                noise_levels = self.model.karras_levels(N)\n",
    "                t1, t2 = self.model.sample_timesteps(time_key, noise_levels, (*x_batch.shape[:2], 1))\n",
    "                \n",
    "                state, loss = self.train_step(state,\n",
    "                                              (x_batch, y_batch),\n",
    "                                              t1, t2,\n",
    "                                              jnp.asarray(train_keys),\n",
    "                                              self.model.consistency_fn,\n",
    "                                              self.loss_fn,\n",
    "                                              self.model.config.time_embed_size,\n",
    "                                              sigma, eps, c_data)\n",
    "                \n",
    "                steps.set_postfix(val=unreplicate(loss))\n",
    "                \n",
    "        self.model.state = unreplicate(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏋️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_key = random.key(0)\n",
    "random_key, model_key, trainer_key = random.split(random_key, 3)\n",
    "\n",
    "model_config = MLPMixerConfig(batch_size=256,\n",
    "                        input_size=28, \n",
    "                        channel_size=1,\n",
    "                        time_embed_size=16)\n",
    "\n",
    "model = ConsistencyModel(model_config, model_key)\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    batch = default_collate(batch)\n",
    "    batch = tree_map(lambda x: np.asarray(x), batch)\n",
    "    return batch\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: x.permute(1, 2, 0)),\n",
    "    Lambda(lambda x: x * 2 - 1),\n",
    "])\n",
    "\n",
    "\n",
    "mnist_dataset = MNIST('/tmp/mnist', download=True, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset=mnist_dataset,\n",
    "                         batch_size=model_config.batch_size,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=numpy_collate,\n",
    "                         drop_last=True)\n",
    "\n",
    "trainer = ConsistencyTrainer(model=model,\n",
    "                             dataloader=dataloader,\n",
    "                             config=TrainerConfig(),\n",
    "                             random_key=trainer_key)\n",
    "\n",
    "trainer.train(timesteps=int(1e4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
