{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency Training, but with JAX ðŸŽ†\n",
    "\n",
    "Improved Consistency Training (iCT) from [arxiv.org/abs/2310.14189](https://arxiv.org/abs/2310.14189), with JAX & Flax, partially based on [smsharma/consistency-models](https://github.com/smsharma/consistency-models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install jax flax torch torchvision ipykernel\n",
    "%pip install ml_collections matplotlib einops wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import einops\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax.training.train_state import TrainState\n",
    "import flax.linen as nn\n",
    "\n",
    "import optax\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from jax.tree_util import tree_map\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional, Sequence, Callable, Any\n",
    "from dataclasses import dataclass\n",
    "from abc import abstractmethod\n",
    "\n",
    "Shape = int | Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.Dense(self.mlp_dim)(x)\n",
    "        y = nn.gelu(y)\n",
    "        return nn.Dense(x.shape[-1])(y)\n",
    "\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        y = MLPBlock(self.tokens_mlp_dim)(y)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        x = x + y\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = MLPBlock(self.channels_mlp_dim)(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    patch_size: int\n",
    "    num_blocks: int\n",
    "    hidden_dim: int\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "    num_classes: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t, context):\n",
    "        b, h, w, c = x.shape\n",
    "\n",
    "        d_t_emb = t.shape[-1]\n",
    "\n",
    "        context = nn.Embed(self.num_classes, t.shape[-1])(context)\n",
    "        context = einops.repeat(context, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "\n",
    "        t = einops.repeat(t, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "        context = jnp.concatenate([context, t], axis=-1)\n",
    "\n",
    "        context = nn.gelu(nn.Dense(self.tokens_mlp_dim)(context))\n",
    "        context = nn.Dense(d_t_emb)(context)\n",
    "\n",
    "        x = jnp.concatenate([x, context], axis=-1)\n",
    "\n",
    "        x = nn.Conv(self.hidden_dim, [self.patch_size, self.patch_size], strides=[self.patch_size, self.patch_size])(x)\n",
    "        x = einops.rearrange(x, \"n h w c -> n (h w) c\")\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = MixerBlock(self.tokens_mlp_dim, self.channels_mlp_dim)(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = nn.Dense(self.patch_size * self.patch_size * c)(x)\n",
    "        x = einops.rearrange(x, \"b (hp wp) (ph pw c) -> b (hp ph) (wp pw) c\", hp=h // self.patch_size, wp=w // self.patch_size, ph=self.patch_size, pw=self.patch_size, c=c)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size: int\n",
    "    input_size: int\n",
    "    channel_size: int\n",
    "    time_embed_size: int\n",
    "    max_time: float = 80.0\n",
    "    s0: int = 2\n",
    "    s1: int = 150\n",
    "    eps: float = 2e-3\n",
    "    sigma: float = 0.5\n",
    "    dtype: Any = jnp.float32\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_backbone(self) -> nn.Module:\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class MLPMixerConfig(ModelConfig):\n",
    "    patch_size: int = 4\n",
    "    num_blocks: int = 4\n",
    "    hidden_dim: int = 256\n",
    "    tokens_mlp_dim: int = 256\n",
    "    channels_mlp_dim: int = 256\n",
    "    num_classes: int = 10\n",
    "    \n",
    "    def _create_backbone(self):\n",
    "        return MLPMixer(patch_size=self.patch_size,\n",
    "                        num_blocks=self.num_blocks,\n",
    "                        hidden_dim=self.hidden_dim,\n",
    "                        tokens_mlp_dim=self.tokens_mlp_dim,\n",
    "                        channels_mlp_dim=self.channels_mlp_dim,\n",
    "                        num_classes=self.num_classes)\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    lr: float = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyModel:\n",
    "    def __init__(self, config: ModelConfig, key: jax.dtypes.prng_key):\n",
    "        self.config = config\n",
    "        self.random_key = key\n",
    "        self.backbone = config._create_backbone()\n",
    "    \n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(1, 4, 5))\n",
    "    def consistency_fn(apply_params: Any,\n",
    "                       apply_fn: Callable,\n",
    "                       x: jax.Array,\n",
    "                       timestep_emb: jax.Array,\n",
    "                       sigma_data: float,\n",
    "                       eps: float):\n",
    "        \n",
    "        cskip = lambda t: sigma_data ** 2 / ((t-eps)**2 + sigma_data ** 2)[:, :, None, None]\n",
    "        cout = lambda t: sigma_data * (t-eps) / jnp.sqrt(t**2 + sigma_data ** 2)[:, :, None, None]\n",
    "\n",
    "        x = apply_fn(apply_params, x, timestep_emb)\n",
    "\n",
    "        return x * cskip(timestep_emb) + x * cout(timestep_emb)\n",
    "\n",
    "    def sample(self, timesteps: Sequence):\n",
    "        initial_variance = self.config.max_time ** 2\n",
    "\n",
    "        self.random_key, normal_key = random.split(self.random_key)\n",
    "        xt = random.normal(normal_key, self.config.dim) * initial_variance\n",
    "\n",
    "        x = self.consistency_fn(xt, self.config.max_time, self.backbone.apply_fn, self.backbone.params, self.config.sigma_data, self.config.eps)\n",
    "\n",
    "        for t in timesteps:\n",
    "            normal_key, _ = random.split(normal_key)\n",
    "            z = random.normal(normal_key, self.config.dim)\n",
    "            xt = x + jnp.sqrt(t**2 - self.config.eps**2) * z\n",
    "            x = self.consistency_fn(xt, self.config.max_time, self.model.apply_fn, self.model.params, self.config.sigma_data, self.config.eps)\n",
    "    \n",
    "    def param_count(self):\n",
    "        if self.train_state is None:\n",
    "            raise \"Model not initialized. Call `create_state` first to initialize.\"\n",
    "        \n",
    "        return sum(x.size for x in jax.tree_leaves(self.state.params))\n",
    "    \n",
    "    def discretize(self, sigma: float, eps: float, N: int):\n",
    "        idx = jnp.arange(N)\n",
    "        return (eps ** (1 / sigma) + idx / (N - 1) * (self.config.max_time ** (1 / sigma) - eps ** (1 / sigma))) ** sigma\n",
    "    \n",
    "    def get_boundaries(self, step: int):\n",
    "        N = (step * ((self.config.s1 + 1) ** 2 - self.config.s0 ** 2) / self.config.max_time)\n",
    "        N = jnp.ceil(jnp.sqrt(N + self.config.s0 ** 2) - 1) + 1\n",
    "        boundaries = self.discretize(self.config.sigma, self.config.eps, N)\n",
    "        return boundaries, N\n",
    "    \n",
    "    def create_state(self, key: jax.dtypes.prng_key, optimizer: Optional[Any] = None, keep_state: bool = True):\n",
    "        noise_key, init_key = random.split(key)\n",
    "        init_shape = (self.config.batch_size,\n",
    "                      self.config.input_size,\n",
    "                      self.config.input_size,\n",
    "                      self.config.channel_size)\n",
    "        x = jax.random.normal(noise_key, init_shape)\n",
    "        t = jnp.ones((self.config.batch_size, self.config.time_embed_size))\n",
    "        \n",
    "        context = jnp.ones((self.config.batch_size,)).astype(jnp.int32)\n",
    "        params = self.backbone.init(init_key, x, t, context)\n",
    "        train_state = TrainState.create(apply_fn=self.backbone.apply, params=params, tx=optimizer)\n",
    "\n",
    "        if keep_state:\n",
    "            self.state = train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConsistencyTrainer:\n",
    "    model: ConsistencyModel\n",
    "    dataloader: DataLoader\n",
    "    config: TrainerConfig\n",
    "    random_key: jax.dtypes.prng_key\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        if isinstance(self.dataloader, DataLoader):\n",
    "            self.dataloader = iter(self.dataloader)\n",
    "\n",
    "        self.random_key, init_key = jax.random.split(self.random_key)\n",
    "        tx = optax.radam(learning_rate=self.config.lr)\n",
    "        self.train_state = self.model.create_state(optimizer=tx, key=init_key)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: loss_fn and train_step are full of shit and aren't iCT.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(5,))\n",
    "    def loss_fn(params, consistency_fn, x, t1, t2, score, key, y):\n",
    "        z = jax.random.normal(key, shape=x.shape)\n",
    "\n",
    "        x2 = x + z * t2[:, :, None, None]\n",
    "        x2 = consistency_fn(params, score, x2, t2, y)\n",
    "\n",
    "        x1 = x + z * t1[:, :, None, None]\n",
    "        x1 = consistency_fn(score, x1, t1, y)\n",
    "\n",
    "        return np.mean((x1 - x2) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(5, 6, 7, 8, 9, 10))\n",
    "    def train_step(train_state: TrainState,\n",
    "                   batch: jax.Array,\n",
    "                   t1: jax.Array,\n",
    "                   t2: jax.Array,\n",
    "                   key: jax.dtypes.prng_key,\n",
    "                   model: ConsistencyModel,\n",
    "                   loss_fn: Callable,\n",
    "                   sigma_data: float,\n",
    "                   eps: float,\n",
    "                   time_embed_size: float):\n",
    "                   \n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "        \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(train_state.params, x_batch, t1, t2, model, key, y_batch, sigma_data, eps, time_embed_size)\n",
    "        grads = jax.lax.pmean(grads, \"batch\")\n",
    "        loss = jax.lax.pmean(loss, \"batch\")\n",
    "\n",
    "        train_state = train_state.apply_gradients(grads=grads)\n",
    "\n",
    "        metrics = {\"loss\": loss}\n",
    "\n",
    "        return train_state, metrics\n",
    "\n",
    "    def train(self, timesteps: int):\n",
    "        state = self.train_state\n",
    "        \n",
    "        with trange(timesteps) as steps:\n",
    "            for step in steps:\n",
    "                self.random_key, time_key, step_key = jax.random.split(self.random_key, 3)\n",
    "                x_batch, y_batch = next(self.dataloader)\n",
    "\n",
    "                boundaries, N = self.model.get_boundaries(step)\n",
    "                n_batch = jax.random.randint(time_key, minval=0, maxval=N - 1, shape=(x_batch.shape[0], 1))\n",
    "\n",
    "                state, loss = self.train_step(state,\n",
    "                                              (x_batch, y_batch),\n",
    "                                              boundaries[n_batch],\n",
    "                                              boundaries[n_batch + 1],\n",
    "                                              step_key,\n",
    "                                              score,\n",
    "                                              self.loss_fn)\n",
    "\n",
    "                steps.set_postfix(val=loss)\n",
    "                \n",
    "        self.train_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 24\u001b[0m\n\u001b[1;32m     14\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mmnist_dataset,\n\u001b[1;32m     15\u001b[0m                          batch_size\u001b[38;5;241m=\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     16\u001b[0m                          shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m                          collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ConsistencyTrainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m                              dataloader\u001b[38;5;241m=\u001b[39mdataloader,\n\u001b[1;32m     21\u001b[0m                              config\u001b[38;5;241m=\u001b[39mTrainerConfig(),\n\u001b[1;32m     22\u001b[0m                              random_key\u001b[38;5;241m=\u001b[39mtrainer_key)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 77\u001b[0m, in \u001b[0;36mConsistencyTrainer.train\u001b[0;34m(self, timesteps)\u001b[0m\n\u001b[1;32m     69\u001b[0m         boundaries, N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_boundaries(step)\n\u001b[1;32m     70\u001b[0m         n_batch \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(time_key, minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, maxval\u001b[38;5;241m=\u001b[39mN \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(x_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     72\u001b[0m         state, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(state,\n\u001b[1;32m     73\u001b[0m                                       (x_batch, y_batch),\n\u001b[1;32m     74\u001b[0m                                       boundaries[n_batch],\n\u001b[1;32m     75\u001b[0m                                       boundaries[n_batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     76\u001b[0m                                       step_key,\n\u001b[0;32m---> 77\u001b[0m                                       \u001b[43mscore\u001b[49m,\n\u001b[1;32m     78\u001b[0m                                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn)\n\u001b[1;32m     80\u001b[0m         steps\u001b[38;5;241m.\u001b[39mset_postfix(val\u001b[38;5;241m=\u001b[39mloss)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state \u001b[38;5;241m=\u001b[39m state\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "random_key = random.key(0)\n",
    "random_key, model_key, trainer_key = random.split(random_key, 3)\n",
    "\n",
    "model_config = MLPMixerConfig(batch_size=4,\n",
    "                        input_size=28, \n",
    "                        channel_size=1,\n",
    "                        time_embed_size=16)\n",
    "\n",
    "model = ConsistencyModel(model_config, model_key)\n",
    "\n",
    "transform = Compose([ToTensor()])\n",
    "collate_fn = lambda x: tree_map(np.asarray, default_collate(x))\n",
    "mnist_dataset = MNIST('/tmp/mnist', download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset=mnist_dataset,\n",
    "                         batch_size=model_config.batch_size,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=collate_fn)\n",
    "\n",
    "trainer = ConsistencyTrainer(model=model,\n",
    "                             dataloader=dataloader,\n",
    "                             config=TrainerConfig(),\n",
    "                             random_key=trainer_key)\n",
    "\n",
    "trainer.train(timesteps=int(1e4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
