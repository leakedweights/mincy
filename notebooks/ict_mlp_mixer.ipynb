{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Techniques for Training Consistency Models\n",
    "\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2310.14189-b31b1b.svg)](https://arxiv.org/abs/2310.14189)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/leakedweights/mincy/blob/main/notebooks/ict_mlp_mixer.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "JAX & Flax implementation of [Improved Consistency Training](https://arxiv.org/abs/2310.14189).\n",
    "This code is partially based on [smsharma/consistency-models](https://github.com/smsharma/consistency-models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision ipykernel einops wandb imageio\n",
    "%pip install --upgrade jax flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import multiprocessing\n",
    "\n",
    "hardware = jax.default_backend()\n",
    "\n",
    "if hardware == \"tpu\":\n",
    "    pass # setup tpu\n",
    "\n",
    "elif hardware == \"gpu\":\n",
    "    pass # setup gpu\n",
    "\n",
    "elif hardware == \"cpu\":\n",
    "    os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(\n",
    "        multiprocessing.cpu_count()\n",
    "    )\n",
    "\n",
    "    jax.config.update('jax_platform_name', 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚚 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 14:50:40.198172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import einops\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from jax.scipy.special import erf\n",
    "\n",
    "import optax\n",
    "import numpy as np\n",
    "from optax.losses import l2_loss\n",
    "from jax.tree_util import tree_map\n",
    "from flax.training import checkpoints\n",
    "from torchvision.datasets import MNIST\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "\n",
    "import wandb\n",
    "import imageio\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from abc import abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎲 Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.Dense(self.mlp_dim)(x)\n",
    "        y = nn.gelu(y)\n",
    "        return nn.Dense(x.shape[-1])(y)\n",
    "\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        y = MLPBlock(self.tokens_mlp_dim)(y)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        x = x + y\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = MLPBlock(self.channels_mlp_dim)(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    patch_size: int\n",
    "    num_blocks: int\n",
    "    hidden_dim: int\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "    num_classes: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t, context):\n",
    "        b, h, w, c = x.shape\n",
    "        d_t_emb = t.shape[-1]\n",
    "\n",
    "        context = nn.Embed(self.num_classes, t.shape[-1])(context)\n",
    "        context = einops.repeat(context, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "\n",
    "        t = einops.repeat(t, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "        context = jnp.concatenate([context, t], axis=-1)\n",
    "\n",
    "        context = nn.gelu(nn.Dense(self.tokens_mlp_dim)(context))\n",
    "        context = nn.Dense(d_t_emb)(context)\n",
    "\n",
    "        x = jnp.concatenate([x, context], axis=-1)\n",
    "\n",
    "        x = nn.Conv(self.hidden_dim, [self.patch_size, self.patch_size], strides=[self.patch_size, self.patch_size])(x)\n",
    "        x = einops.rearrange(x, \"n h w c -> n (h w) c\")\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = MixerBlock(self.tokens_mlp_dim, self.channels_mlp_dim)(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = nn.Dense(self.patch_size * self.patch_size * c)(x)\n",
    "        x = einops.rearrange(x, \"b (hp wp) (ph pw c) -> b (hp ph) (wp pw) c\", hp=h // self.patch_size, wp=w // self.patch_size, ph=self.patch_size, pw=self.patch_size, c=c)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size: int\n",
    "    input_size: int\n",
    "    channel_size: int\n",
    "    time_embed_size: int\n",
    "    max_time: float = 80.0\n",
    "    s0: int = 10\n",
    "    s1: int = 1280\n",
    "    c=5.4e-4\n",
    "    eps: float = 2e-3\n",
    "    sigma: float = 0.5\n",
    "    rho: float = 7.0\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80\n",
    "    p_mean: float = -1.1\n",
    "    p_std: float = 2.0\n",
    "    dtype: Any = jnp.float32\n",
    "\n",
    "    def __post_init__(self):\n",
    "        data_dim = self.channel_size * self.input_size ** 2\n",
    "        self.c_data = float(self.c * jnp.sqrt(data_dim))\n",
    "        self.device_batch_size = self.batch_size // jax.device_count()\n",
    "        self.init_shape = (self.device_batch_size,\n",
    "                      self.input_size,\n",
    "                      self.input_size,\n",
    "                      self.channel_size)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_backbone(self) -> nn.Module:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MLPMixerConfig(ModelConfig):\n",
    "    patch_size: int = 4\n",
    "    num_blocks: int = 4\n",
    "    hidden_dim: int = 256\n",
    "    tokens_mlp_dim: int = 256\n",
    "    channels_mlp_dim: int = 256\n",
    "    num_classes: int = 10\n",
    "    \n",
    "    def _create_backbone(self):\n",
    "        return MLPMixer(patch_size=self.patch_size,\n",
    "                        num_blocks=self.num_blocks,\n",
    "                        hidden_dim=self.hidden_dim,\n",
    "                        tokens_mlp_dim=self.tokens_mlp_dim,\n",
    "                        channels_mlp_dim=self.channels_mlp_dim,\n",
    "                        num_classes=self.num_classes)\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    lr: float = 3e-4\n",
    "    log_wandb: bool = True\n",
    "    log_granularity: int = 100\n",
    "    generation_granularity: int = 1000\n",
    "    generation_save_path = \"/tmp/mincy/train_samples\"\n",
    "    generation_timesteps = [500, 100, 10]\n",
    "    generation_classes = 1\n",
    "    ckpt_granularity: int = 5000\n",
    "    ckpt_path: str = \"/tmp/mincy/checkpoints\"\n",
    "    ckpts_to_keep: int = 1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.ckpt_path = os.path.abspath(self.ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✨ Consistency Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_huber_loss(x: jax.Array, y: jax.Array, c_data: float):\n",
    "    loss = l2_loss(x, y)\n",
    "    loss = jnp.sqrt(loss + c_data**2) - c_data\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(time: jax.Array, embedding_size: int):\n",
    "    time = time[..., 0]\n",
    "    time = time * 1e3\n",
    "    half_dim = embedding_size // 2\n",
    "    emb_scale = jnp.log(1e4) / (half_dim - 1)\n",
    "    \n",
    "    emb = jnp.arange(half_dim) * -emb_scale\n",
    "    emb = jnp.exp(emb)\n",
    "    emb = emb[None, :] * time[:, None]\n",
    "\n",
    "    sin_emb = jnp.sin(emb)\n",
    "    cos_emb = jnp.cos(emb)\n",
    "    embedding = jnp.concatenate([sin_emb, cos_emb], axis=-1)\n",
    "\n",
    "    if embedding_size % 2 == 1:\n",
    "        padding = ((0, 0), (0, 0), (0, 1))\n",
    "        embedding = jnp.pad(embedding, padding, mode='constant')\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyModel:\n",
    "    def __init__(self, config: ModelConfig, key: jax.dtypes.prng_key):\n",
    "        self.config = config\n",
    "        self.random_key = key\n",
    "        self.backbone = config._create_backbone()\n",
    "    \n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(3, 4))\n",
    "    def consistency_fn(x: jax.Array,\n",
    "                       h: jax.Array,\n",
    "                       t: float,\n",
    "                       sigma: float,\n",
    "                       eps: float):\n",
    "        \n",
    "        cskip = lambda t: (sigma ** 2 / ((t-eps)**2 + sigma ** 2))[:, :, None, None]\n",
    "        cout = lambda t: (sigma * (t-eps) / jnp.sqrt(t**2 + sigma ** 2))[:, :, None, None]\n",
    "        return x * cskip(t) + h * cout(t)\n",
    "    \n",
    "    def get_param_count(self):\n",
    "        if self.train_state is None:\n",
    "            raise \"Model not initialized. Call `create_state` first to initialize.\"\n",
    "        \n",
    "        return sum(x.size for x in jax.tree_leaves(self.state.params))\n",
    "    \n",
    "    def ict_discretize(self, step: int, max_steps):\n",
    "        k_prime = jnp.floor(max_steps / (jnp.log2(jnp.floor(self.config.s1 / self.config.s0)) + 1))\n",
    "        N = self.config.s0 * jnp.pow(2, jnp.floor(step / k_prime))\n",
    "        N = N.at[N > self.config.s1].set(self.config.s1)\n",
    "        return N + 1\n",
    "\n",
    "    def karras_levels(self, N):\n",
    "            rho = self.config.rho\n",
    "            sigma_min = self.config.sigma_min\n",
    "            sigma_max = self.config.sigma_max\n",
    "            idx = jnp.arange(0, N-1)\n",
    "\n",
    "            sigma_i = jnp.pow(sigma_min, 1.0 / rho) \n",
    "            sigma_i = sigma_i + (idx-1) / (N-1) * (jnp.pow(sigma_max, 1.0 / rho) - jnp.pow(sigma_min, 1.0 / rho) )\n",
    "            sigma_i = jnp.pow(sigma_i, rho)\n",
    "            return sigma_i\n",
    "\n",
    "    def sample_timesteps(self, key, noise_levels, shape):\n",
    "            sigma_erf = lambda sigma: (jnp.log(sigma[1:]) - self.config.p_mean) / (jnp.sqrt(2) * self.config.p_std)\n",
    "\n",
    "            index_erfs = sigma_erf(noise_levels[1:]) - sigma_erf(noise_levels[:-1])\n",
    "            probs = index_erfs / jnp.sum(index_erfs)\n",
    "            timesteps = random.choice(key, len(probs), p=probs, replace=True, shape=shape)\n",
    "            \n",
    "            t1 = noise_levels[timesteps]\n",
    "            t2 = noise_levels[timesteps + 1]\n",
    "            return t1, t2\n",
    "    \n",
    "    def sample(self, timesteps, target_classes):\n",
    "        self.random_key, noise_key = random.split(self.random_key)\n",
    "        noise_shape = self.config.init_shape\n",
    "        batch_timestep = einops.rearrange(jnp.repeat(timesteps[0], self.config.device_batch_size), \"b -> b 1\")\n",
    "        timestep_embedding = sinusoidal_embedding(batch_timestep, self.config.time_embed_size)\n",
    "        x = jax.random.normal(noise_key, noise_shape) * timesteps[0]\n",
    "\n",
    "        h = self.state.apply_fn(self.state.params, x, timestep_embedding, target_classes)\n",
    "        x = self.consistency_fn(x, h , batch_timestep, self.config.sigma, self.config.eps)\n",
    "\n",
    "        for timestep in timesteps[1:]:\n",
    "            noise_key, _ = random.split(noise_key)\n",
    "            noise = jax.random.normal(noise_key, noise_shape)\n",
    "            batch_timestep = einops.rearrange(jnp.repeat(timestep, self.config.device_batch_size), \"b -> b 1\")\n",
    "\n",
    "            timestep_embedding = sinusoidal_embedding(batch_timestep, self.config.time_embed_size)\n",
    "            x_noisy = x + jnp.sqrt(timestep**2 - self.config.eps**2) * noise\n",
    "            h = self.state.apply_fn(self.state.params, x_noisy, timestep_embedding, target_classes)\n",
    "            x = self.consistency_fn(x, h , batch_timestep, self.config.sigma, self.config.eps)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def create_state(self, key: jax.dtypes.prng_key, optimizer: Optional[Any] = None):\n",
    "        noise_key, init_key = random.split(key)\n",
    "        x = jax.random.normal(noise_key, self.config.init_shape)\n",
    "        t = jnp.ones((self.config.device_batch_size, self.config.time_embed_size))\n",
    "        \n",
    "        y = jnp.ones((self.config.device_batch_size,)).astype(jnp.int32)\n",
    "        params = self.backbone.init(init_key, x, t, y)\n",
    "        train_state = TrainState.create(apply_fn=self.backbone.apply, params=params, tx=optimizer)\n",
    "        self.state = train_state\n",
    "\n",
    "        return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyTrainer:\n",
    "    def __init__(self, model: ConsistencyModel, dataloader: DataLoader, config: TrainerConfig, random_key: jax.dtypes.prng_key):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.config = config\n",
    "        self.random_key = random_key\n",
    "        self.random_key, init_key = jax.random.split(self.random_key)\n",
    "\n",
    "        tx = optax.radam(learning_rate=self.config.lr)\n",
    "        self.model_state = self.model.create_state(optimizer=tx, key=init_key)\n",
    "        \n",
    "        if not os.path.exists(self.config.ckpt_path):\n",
    "            os.makedirs(self.config.ckpt_path)\n",
    "\n",
    "        if not os.path.exists(self.config.generation_save_path):\n",
    "            os.makedirs(self.config.generation_save_path)\n",
    "\n",
    "    def handle_ckpt(self, step: int, state: TrainState):\n",
    "        if step % self.config.ckpt_granularity == self.config.ckpt_granularity - 1:\n",
    "            checkpoints.save_checkpoint(ckpt_dir=self.config.ckpt_path,\n",
    "                              target=state,\n",
    "                              step=step,\n",
    "                              overwrite=True,\n",
    "                              keep=self.config.ckpts_to_keep)\n",
    "                  \n",
    "\n",
    "    def restore_ckpt(self, step: Optional[int] = None, load=True):\n",
    "        restored_state = checkpoints.restore_checkpoint(ckpt_dir=self.config.ckpt_path,\n",
    "                                                        target=self.model.state,\n",
    "                                                        step=step)\n",
    "        if load:\n",
    "            self.model.state = restored_state\n",
    "        return restored_state\n",
    "    \n",
    "    def handle_metrics(self, step: int, metrics: dict):\n",
    "        if not self.config.log_wandb:\n",
    "            return\n",
    "        \n",
    "        if step % self.config.log_granularity == self.config.log_granularity - 1:\n",
    "            wandb.log(metrics, step=step)\n",
    "\n",
    "    def handle_sampling(self, step: int, state: TrainState):\n",
    "        if step % self.config.generation_granularity == self.config.generation_granularity - 1:\n",
    "            self.model.state = state\n",
    "            target_classes = jnp.repeat(self.config.generation_classes, self.model.config.device_batch_size)\n",
    "            outputs = self.model.sample(self.config.generation_timesteps, target_classes)\n",
    "            \n",
    "            for i, image in enumerate(outputs):\n",
    "                image_path = os.path.join(self.config.generation_save_path, f\"sample_{step}_{i}.png\")\n",
    "                imageio.imwrite(image_path, image)\n",
    "                print(f\"Saved image to {image_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(1, 2, 6, 9, 10, 11))\n",
    "    def loss_fn(params, apply_fn, consistency_fn, x, t1, t2, time_embed_size, y, key, sigma, eps, c_data):\n",
    "        z = jax.random.normal(key, shape=x.shape)\n",
    "\n",
    "        x1 = x + z * t1[:, :, None, None]\n",
    "        t1_emb = sinusoidal_embedding(t1, time_embed_size)\n",
    "\n",
    "        h1 = jax.lax.stop_gradient(apply_fn(params, x, t1_emb, y)) # theta^-\n",
    "        x1 = consistency_fn(x1, h1, t1, sigma, eps)\n",
    "\n",
    "        x2 = x + z * t2[:, :, None, None]\n",
    "        t2_emb = sinusoidal_embedding(t2, time_embed_size)\n",
    "\n",
    "        h2 = apply_fn(params, x, t2_emb, y)\n",
    "        x2 = consistency_fn(x2, h2, t2, sigma, eps)\n",
    "\n",
    "        loss = pseudo_huber_loss(x1, x2, c_data)\n",
    "        weight = (1 / (t2 - t1))[:, :, None, None]\n",
    "\n",
    "        return jnp.mean(weight * loss)\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(5, 6, 7, 8, 9, 10))\n",
    "    def train_step(state, batch, t1, t2, key, consistency_fn, loss_fn, time_embed_size, sigma, eps, c_data):\n",
    "        x, y = batch\n",
    "        params = state.params\n",
    "        apply_fn = state.apply_fn\n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, apply_fn, consistency_fn, x, t1, t2, time_embed_size, y, key, sigma, eps, c_data)\n",
    "\n",
    "        grads = jax.lax.pmean(grads, \"batch\")\n",
    "        loss = jax.lax.pmean(loss, \"batch\")\n",
    "\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss\n",
    "\n",
    "    def train(self, timesteps: int):\n",
    "        state = replicate(self.model.state)\n",
    "        batch_size = self.model.config.batch_size\n",
    "        num_devices = jax.device_count()\n",
    "        device_batch_size = batch_size // num_devices\n",
    "\n",
    "        assert batch_size % num_devices == 0, \\\n",
    "            f\"Batch size must be divisible by the number of devices, but got {batch_size} and {num_devices}.\"\n",
    "        \n",
    "        with trange(timesteps) as steps:\n",
    "            for step in steps:\n",
    "                try:\n",
    "                    batch = next(self.dataloader.__iter__())\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "                \n",
    "                x_batch, y_batch = batch\n",
    "\n",
    "                b, h, w, c = x_batch.shape\n",
    "                x_batch = x_batch.reshape(num_devices, device_batch_size, h, w, c)\n",
    "                y_batch = y_batch.reshape(num_devices, device_batch_size)\n",
    "\n",
    "                sigma = self.model.config.sigma\n",
    "                eps = self.model.config.eps\n",
    "                c_data = self.model.config.c_data\n",
    "                self.random_key, time_key = random.split(self.random_key)\n",
    "                self.random_key, *train_keys = random.split(self.random_key, min(x_batch.shape[0], jax.local_device_count()) + 1)\n",
    "\n",
    "                N = self.model.ict_discretize(step, timesteps)\n",
    "                noise_levels = self.model.karras_levels(N)\n",
    "                t1, t2 = self.model.sample_timesteps(time_key, noise_levels, (*x_batch.shape[:2], 1))\n",
    "                \n",
    "                state, loss = self.train_step(state,\n",
    "                                              (x_batch, y_batch),\n",
    "                                              t1, t2,\n",
    "                                              jnp.asarray(train_keys),\n",
    "                                              self.model.consistency_fn,\n",
    "                                              self.loss_fn,\n",
    "                                              self.model.config.time_embed_size,\n",
    "                                              sigma, eps, c_data)\n",
    "                loss = unreplicate(loss)\n",
    "                steps.set_postfix(val=loss)\n",
    "                self.handle_ckpt(step, unreplicate(state))\n",
    "                self.handle_metrics(step, {\"training loss\": loss})\n",
    "                self.handle_sampling(step, unreplicate(state))\n",
    "                \n",
    "        self.model.state = unreplicate(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏋️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"minimal-consistency-jax\",\n",
    "    job_type=\"simple-train-loop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_key = random.key(0)\n",
    "random_key, model_key, trainer_key = random.split(random_key, 3)\n",
    "\n",
    "model_config = MLPMixerConfig(batch_size=512,\n",
    "                        input_size=28, \n",
    "                        channel_size=1,\n",
    "                        time_embed_size=16)\n",
    "\n",
    "model = ConsistencyModel(model_config, model_key)\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    batch = default_collate(batch)\n",
    "    batch = tree_map(lambda x: np.asarray(x), batch)\n",
    "    return batch\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: x.permute(1, 2, 0)),\n",
    "    Lambda(lambda x: x * 2 - 1),\n",
    "])\n",
    "\n",
    "\n",
    "mnist_dataset = MNIST('/tmp/mnist', download=True, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset=mnist_dataset,\n",
    "                         batch_size=model_config.batch_size,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=numpy_collate,\n",
    "                         drop_last=True)\n",
    "\n",
    "trainer = ConsistencyTrainer(model=model,\n",
    "                             dataloader=dataloader,\n",
    "                             config=TrainerConfig(),\n",
    "                             random_key=trainer_key)\n",
    "\n",
    "trainer.train(timesteps=int(1e4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
