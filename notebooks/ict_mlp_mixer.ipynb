{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Techniques for Training Consistency Models\n",
    "\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2310.14189-b31b1b.svg)](https://arxiv.org/abs/2310.14189)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/leakedweights/mincy/blob/main/notebooks/ict_mlp_mixer.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "JAX & Flax implementation of [Improved Consistency Training](https://arxiv.org/abs/2310.14189).\n",
    "This code is partially based on [smsharma/consistency-models](https://github.com/smsharma/consistency-models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (0.4.26)\n",
      "Requirement already satisfied: flax in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (0.8.2)\n",
      "Collecting orbax\n",
      "  Using cached orbax-0.1.9.tar.gz (1.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: ipykernel in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (6.29.4)\n",
      "Requirement already satisfied: einops in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: wandb in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (0.16.6)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from jax) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from jax) (1.13.0)\n",
      "Requirement already satisfied: opt-einsum in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from jax) (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from flax) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from flax) (4.11.0)\n",
      "Requirement already satisfied: tensorstore in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from flax) (0.1.56)\n",
      "Requirement already satisfied: optax in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from flax) (0.2.2)\n",
      "Requirement already satisfied: rich>=11.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from flax) (13.7.1)\n",
      "Requirement already satisfied: orbax-checkpoint in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from flax) (0.5.9)\n",
      "Requirement already satisfied: msgpack in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from flax) (1.0.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: filelock in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (2.3.0)\n",
      "Requirement already satisfied: sympy in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: psutil in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (26.0.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: packaging in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (24.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (1.8.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (8.23.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipykernel) (8.6.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (1.45.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: setuptools in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from wandb) (59.6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.17.2)\n",
      "Requirement already satisfied: exceptiongroup in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (1.2.1)\n",
      "Requirement already satisfied: decorator in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.0)\n",
      "Requirement already satisfied: jaxlib in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from orbax-checkpoint->flax) (0.4.26)\n",
      "Requirement already satisfied: etils[epath,epy] in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.7.0)\n",
      "Requirement already satisfied: absl-py in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from orbax-checkpoint->flax) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: chex>=0.1.86 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from optax->flax) (0.1.86)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from chex>=0.1.86->optax->flax) (0.12.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: zipp in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.18.1)\n",
      "Requirement already satisfied: importlib_resources in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.4.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in /home/betonitcso/projects/mincy/.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "Building wheels for collected packages: orbax\n",
      "  Building wheel for orbax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for orbax: filename=orbax-0.1.9-py3-none-any.whl size=1531 sha256=994fb76a895caee0b6abbc00feaa6528e57d172c44de1a51279076f11d9181b3\n",
      "  Stored in directory: /home/betonitcso/.cache/pip/wheels/14/7a/98/b955a4db98b54317c311ee32367994ca530721c62a87ec56a7\n",
      "Successfully built orbax\n",
      "Installing collected packages: orbax\n",
      "Successfully installed orbax-0.1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision ipykernel einops wandb\n",
    "%pip install --upgrade jax flax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöö Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 15:54:54.862960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import einops\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from jax.scipy.special import erf\n",
    "from jax.scipy.stats import multinomial\n",
    "\n",
    "import optax\n",
    "import numpy as np\n",
    "from optax.losses import l2_loss\n",
    "from jax.tree_util import tree_map\n",
    "from flax.training import checkpoints\n",
    "from torchvision.datasets import MNIST\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "\n",
    "import wandb\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from abc import abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.Dense(self.mlp_dim)(x)\n",
    "        y = nn.gelu(y)\n",
    "        return nn.Dense(x.shape[-1])(y)\n",
    "\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        y = MLPBlock(self.tokens_mlp_dim)(y)\n",
    "        y = jnp.swapaxes(y, 1, 2)\n",
    "        x = x + y\n",
    "        y = nn.LayerNorm()(x)\n",
    "        y = MLPBlock(self.channels_mlp_dim)(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    patch_size: int\n",
    "    num_blocks: int\n",
    "    hidden_dim: int\n",
    "    tokens_mlp_dim: int\n",
    "    channels_mlp_dim: int\n",
    "    num_classes: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t, context):\n",
    "        b, h, w, c = x.shape\n",
    "        d_t_emb = t.shape[-1]\n",
    "\n",
    "        context = nn.Embed(self.num_classes, t.shape[-1])(context)\n",
    "        context = einops.repeat(context, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "\n",
    "        t = einops.repeat(t, \"b t -> b (h p1) (w p2) t\", h=h // self.patch_size, w=w // self.patch_size, p1=self.patch_size, p2=self.patch_size)\n",
    "        context = jnp.concatenate([context, t], axis=-1)\n",
    "\n",
    "        context = nn.gelu(nn.Dense(self.tokens_mlp_dim)(context))\n",
    "        context = nn.Dense(d_t_emb)(context)\n",
    "\n",
    "        x = jnp.concatenate([x, context], axis=-1)\n",
    "\n",
    "        x = nn.Conv(self.hidden_dim, [self.patch_size, self.patch_size], strides=[self.patch_size, self.patch_size])(x)\n",
    "        x = einops.rearrange(x, \"n h w c -> n (h w) c\")\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = MixerBlock(self.tokens_mlp_dim, self.channels_mlp_dim)(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = nn.Dense(self.patch_size * self.patch_size * c)(x)\n",
    "        x = einops.rearrange(x, \"b (hp wp) (ph pw c) -> b (hp ph) (wp pw) c\", hp=h // self.patch_size, wp=w // self.patch_size, ph=self.patch_size, pw=self.patch_size, c=c)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import multiprocessing\n",
    "\n",
    "# hardware = \"tpu\"\n",
    "# hardware = \"gpu\"\n",
    "# hardware = \"cpu\"\n",
    "\n",
    "if hardware == \"tpu\":\n",
    "    pass # setup tpu\n",
    "\n",
    "elif hardware == \"gpu\":\n",
    "    pass # setup gpu\n",
    "\n",
    "elif hardware == \"cpu\":\n",
    "    os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(\n",
    "        multiprocessing.cpu_count()\n",
    "    )\n",
    "\n",
    "    jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "else:\n",
    "  raise Exception(\"Device type must be specified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    batch_size: int\n",
    "    input_size: int\n",
    "    channel_size: int\n",
    "    time_embed_size: int\n",
    "    max_time: float = 80.0\n",
    "    s0: int = 10\n",
    "    s1: int = 1280\n",
    "    c=5.4e-4\n",
    "    eps: float = 2e-3\n",
    "    sigma: float = 0.5\n",
    "    rho: float = 7.0\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80\n",
    "    p_mean: float = -1.1\n",
    "    p_std: float = 2.0\n",
    "    dtype: Any = jnp.float32\n",
    "\n",
    "    def __post_init__(self):\n",
    "        data_dim = self.channel_size * self.input_size ** 2\n",
    "        self.c_data = float(self.c * jnp.sqrt(data_dim))\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_backbone(self) -> nn.Module:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MLPMixerConfig(ModelConfig):\n",
    "    patch_size: int = 4\n",
    "    num_blocks: int = 4\n",
    "    hidden_dim: int = 256\n",
    "    tokens_mlp_dim: int = 256\n",
    "    channels_mlp_dim: int = 256\n",
    "    num_classes: int = 10\n",
    "    \n",
    "    def _create_backbone(self):\n",
    "        return MLPMixer(patch_size=self.patch_size,\n",
    "                        num_blocks=self.num_blocks,\n",
    "                        hidden_dim=self.hidden_dim,\n",
    "                        tokens_mlp_dim=self.tokens_mlp_dim,\n",
    "                        channels_mlp_dim=self.channels_mlp_dim,\n",
    "                        num_classes=self.num_classes)\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    lr: float = 3e-4\n",
    "    log_wandb: bool = True\n",
    "    ckpt_granularity: int = 1\n",
    "    ckpt_path: str = \"/tmp/mincy\"\n",
    "    ckpts_to_keep: int = 1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.ckpt_path = os.path.abspath(self.ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ú® Consistency Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_huber_loss(x: jax.Array, y: jax.Array, c_data: float):\n",
    "    loss = l2_loss(x, y)\n",
    "    loss = jnp.sqrt(loss + c_data**2) - c_data\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(time: jax.Array, embedding_size: int):\n",
    "    time = time[..., 0]\n",
    "    time = time * 1e3\n",
    "    half_dim = embedding_size // 2\n",
    "    emb_scale = jnp.log(1e4) / (half_dim - 1)\n",
    "    \n",
    "    emb = jnp.arange(half_dim) * -emb_scale\n",
    "    emb = jnp.exp(emb)\n",
    "    emb = emb[None, :] * time[:, None]\n",
    "\n",
    "    sin_emb = jnp.sin(emb)\n",
    "    cos_emb = jnp.cos(emb)\n",
    "    embedding = jnp.concatenate([sin_emb, cos_emb], axis=-1)\n",
    "\n",
    "    if embedding_size % 2 == 1:\n",
    "        padding = ((0, 0), (0, 0), (0, 1))\n",
    "        embedding = jnp.pad(embedding, padding, mode='constant')\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyModel:\n",
    "    def __init__(self, config: ModelConfig, key: jax.dtypes.prng_key):\n",
    "        self.config = config\n",
    "        self.random_key = key\n",
    "        self.backbone = config._create_backbone()\n",
    "    \n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(3, 4))\n",
    "    def consistency_fn(x: jax.Array,\n",
    "                       h: jax.Array,\n",
    "                       t: float,\n",
    "                       sigma: float,\n",
    "                       eps: float):\n",
    "        \n",
    "        cskip = lambda t: (sigma ** 2 / ((t-eps)**2 + sigma ** 2))[:, :, None, None]\n",
    "        cout = lambda t: (sigma * (t-eps) / jnp.sqrt(t**2 + sigma ** 2))[:, :, None, None]\n",
    "        return x * cskip(t) + h * cout(t)\n",
    "    \n",
    "    def get_param_count(self):\n",
    "        if self.train_state is None:\n",
    "            raise \"Model not initialized. Call `create_state` first to initialize.\"\n",
    "        \n",
    "        return sum(x.size for x in jax.tree_leaves(self.state.params))\n",
    "    \n",
    "    def ict_discretize(self, step: int):\n",
    "        u = jnp.log(jnp.floor(self.config.s1 / self.config.s0)) + 1\n",
    "        k_prime = jnp.floor(self.config.max_time / u)\n",
    "\n",
    "        N = self.config.s0 * jnp.pow(2, jnp.floor(step / k_prime))\n",
    "        N = min(self.config.s1, N)\n",
    "\n",
    "        return int(N)\n",
    "\n",
    "    def karras_levels(self, N):\n",
    "            rho = self.config.rho\n",
    "            sigma_min = self.config.sigma_min\n",
    "            sigma_max = self.config.sigma_max\n",
    "            idx = jnp.arange(0, N-1)\n",
    "\n",
    "            sigma_i = jnp.pow(sigma_min, 1.0 / rho) \n",
    "            sigma_i = sigma_i + (idx-1) / (N-1) * (jnp.pow(sigma_max, 1.0 / rho) - jnp.pow(sigma_min, 1.0 / rho) )\n",
    "            sigma_i = jnp.pow(sigma_i, rho)\n",
    "            return sigma_i\n",
    "\n",
    "    def sample_timesteps(self, key, noise_levels, shape):\n",
    "            sigma_erf = lambda sigma: (jnp.log(sigma[1:]) - self.config.p_mean) / (jnp.sqrt(2) * self.config.p_std)\n",
    "\n",
    "            index_erfs = sigma_erf(noise_levels[1:]) - sigma_erf(noise_levels[:-1])\n",
    "            probs = index_erfs / jnp.sum(index_erfs)\n",
    "            timesteps = random.choice(key, len(probs), p=probs, replace=True, shape=shape)\n",
    "            \n",
    "            t1 = noise_levels[timesteps]\n",
    "            t2 = noise_levels[timesteps + 1]\n",
    "            return t1, t2\n",
    "    \n",
    "    def create_state(self, key: jax.dtypes.prng_key, optimizer: Optional[Any] = None):\n",
    "        noise_key, init_key = random.split(key)\n",
    "        device_batch_size = self.config.batch_size// jax.device_count()\n",
    "        init_shape = (device_batch_size,\n",
    "                      self.config.input_size,\n",
    "                      self.config.input_size,\n",
    "                      self.config.channel_size)\n",
    "        x = jax.random.normal(noise_key, init_shape)\n",
    "        t = jnp.ones((device_batch_size, self.config.time_embed_size))\n",
    "        \n",
    "        y = jnp.ones((device_batch_size,)).astype(jnp.int32)\n",
    "        params = self.backbone.init(init_key, x, t, y)\n",
    "        train_state = TrainState.create(apply_fn=self.backbone.apply, params=params, tx=optimizer)\n",
    "        self.state = train_state\n",
    "\n",
    "        return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyTrainer:\n",
    "    def __init__(self, model, dataloader: DataLoader, config: TrainerConfig, random_key: jax.dtypes.prng_key):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.config = config\n",
    "        self.random_key = random_key\n",
    "        self.random_key, init_key = jax.random.split(self.random_key)\n",
    "\n",
    "        tx = optax.radam(learning_rate=self.config.lr)\n",
    "        self.model_state = self.model.create_state(optimizer=tx, key=init_key)\n",
    "\n",
    "        if not os.path.exists(self.config.ckpt_path):\n",
    "            os.makedirs(self.config.ckpt_path)\n",
    "\n",
    "    def handle_ckpt(self, step: int, state: TrainState):\n",
    "        if step % self.config.ckpt_granularity == self.config.ckpt_granularity - 1:\n",
    "            checkpoints.save_checkpoint(ckpt_dir=self.config.ckpt_path,\n",
    "                              target=state,\n",
    "                              step=step,\n",
    "                              overwrite=True,\n",
    "                              keep=self.config.ckpts_to_keep)\n",
    "                  \n",
    "\n",
    "    def restore_ckpt(self, step: Optional[int] = None, load=True):\n",
    "        self.random_key, load_key = random.split(self.random_key)\n",
    "        restored_state = checkpoints.restore_checkpoint(ckpt_dir=self.config.ckpt_path,\n",
    "                                                        target=self.model_state,\n",
    "                                                        step=step)\n",
    "        if load:\n",
    "            self.model.state = restored_state\n",
    "            self.model_state = restored_state\n",
    "        return restored_state\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(1, 2, 6, 9, 10, 11))\n",
    "    def loss_fn(params, apply_fn, consistency_fn, x, t1, t2, time_embed_size, y, key, sigma, eps, c_data):\n",
    "        z = jax.random.normal(key, shape=x.shape)\n",
    "\n",
    "        x1 = x + z * t1[:, :, None, None]\n",
    "        t1_emb = sinusoidal_embedding(t1, time_embed_size)\n",
    "\n",
    "        h1 = jax.lax.stop_gradient(apply_fn(params, x, t1_emb, y)) # theta^-\n",
    "        x1 = consistency_fn(x1, h1, t1, sigma, eps)\n",
    "\n",
    "        x2 = x + z * t2[:, :, None, None]\n",
    "        t2_emb = sinusoidal_embedding(t2, time_embed_size)\n",
    "\n",
    "        h2 = apply_fn(params, x, t2_emb, y)\n",
    "        x2 = consistency_fn(x2, h2, t2, sigma, eps)\n",
    "\n",
    "        loss = pseudo_huber_loss(x1, x2, c_data)\n",
    "        weight = (1 / (t2 - t1))[:, :, None, None]\n",
    "\n",
    "        return jnp.mean(weight * loss)\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(5, 6, 7, 8, 9, 10))\n",
    "    def train_step(state, batch, t1, t2, key, consistency_fn, loss_fn, time_embed_size, sigma, eps, c_data):\n",
    "        x, y = batch\n",
    "        params = state.params\n",
    "        apply_fn = state.apply_fn\n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, apply_fn, consistency_fn, x, t1, t2, time_embed_size, y, key, sigma, eps, c_data)\n",
    "\n",
    "        grads = jax.lax.pmean(grads, \"batch\")\n",
    "        loss = jax.lax.pmean(loss, \"batch\")\n",
    "\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss\n",
    "\n",
    "    def train(self, timesteps: int):\n",
    "        state = replicate(self.model.state)\n",
    "        batch_size = self.model.config.batch_size\n",
    "        num_devices = jax.device_count()\n",
    "        device_batch_size = batch_size // num_devices\n",
    "\n",
    "        assert batch_size % num_devices == 0, \\\n",
    "            f\"Batch size must be divisible by the number of devices, but got {batch_size} and {num_devices}.\"\n",
    "        \n",
    "        with trange(timesteps) as steps:\n",
    "            for step in steps:\n",
    "                try:\n",
    "                    batch = next(self.dataloader.__iter__())\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "                \n",
    "                x_batch, y_batch = batch\n",
    "\n",
    "                b, h, w, c = x_batch.shape\n",
    "                x_batch = x_batch.reshape(num_devices, device_batch_size, h, w, c)\n",
    "                y_batch = y_batch.reshape(num_devices, device_batch_size)\n",
    "\n",
    "                sigma = self.model.config.sigma\n",
    "                eps = self.model.config.eps\n",
    "                c_data = self.model.config.c_data\n",
    "                self.random_key, time_key = random.split(self.random_key)\n",
    "                self.random_key, *train_keys = random.split(self.random_key, min(x_batch.shape[0], jax.local_device_count()) + 1)\n",
    "\n",
    "                N = self.model.ict_discretize(step)\n",
    "                noise_levels = self.model.karras_levels(N)\n",
    "                t1, t2 = self.model.sample_timesteps(time_key, noise_levels, (*x_batch.shape[:2], 1))\n",
    "                \n",
    "                state, loss = self.train_step(state,\n",
    "                                              (x_batch, y_batch),\n",
    "                                              t1, t2,\n",
    "                                              jnp.asarray(train_keys),\n",
    "                                              self.model.consistency_fn,\n",
    "                                              self.loss_fn,\n",
    "                                              self.model.config.time_embed_size,\n",
    "                                              sigma, eps, c_data)\n",
    "                loss = unreplicate(loss)\n",
    "                steps.set_postfix(val=loss)\n",
    "\n",
    "                self.handle_ckpt(step, unreplicate(state))\n",
    "\n",
    "                if self.config.log_wandb:\n",
    "                    wandb.log({\n",
    "                        \"Train Loss\": loss,\n",
    "                    }, step=step)\n",
    "                \n",
    "        self.model.state = unreplicate(state)\n",
    "        self.ckpt_manager.wait_until_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"minimal-consistency-jax\",\n",
    "    job_type=\"simple-train-loop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_key = random.key(0)\n",
    "random_key, model_key, trainer_key = random.split(random_key, 3)\n",
    "\n",
    "model_config = MLPMixerConfig(batch_size=256,\n",
    "                        input_size=28, \n",
    "                        channel_size=1,\n",
    "                        time_embed_size=16)\n",
    "\n",
    "model = ConsistencyModel(model_config, model_key)\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    batch = default_collate(batch)\n",
    "    batch = tree_map(lambda x: np.asarray(x), batch)\n",
    "    return batch\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: x.permute(1, 2, 0)),\n",
    "    Lambda(lambda x: x * 2 - 1),\n",
    "])\n",
    "\n",
    "\n",
    "mnist_dataset = MNIST('/tmp/mnist', download=True, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset=mnist_dataset,\n",
    "                         batch_size=model_config.batch_size,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=numpy_collate,\n",
    "                         drop_last=True)\n",
    "\n",
    "trainer = ConsistencyTrainer(model=model,\n",
    "                             dataloader=dataloader,\n",
    "                             config=TrainerConfig(),\n",
    "                             random_key=trainer_key)\n",
    "\n",
    "trainer.train(timesteps=int(1e5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
